{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa64384",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 04: Data Acquisition and Ingestion\n",
    "Name: \n",
    "Date: \n",
    "\n",
    "## Objectives\n",
    "- API ingestion with secrets in `.env`\n",
    "- Scrape a permitted public table\n",
    "- Validate and save raw data to `data/raw/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537640af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      7\u001b[39m RAW = pathlib.Path(\u001b[33m'\u001b[39m\u001b[33mdata/raw\u001b[39m\u001b[33m'\u001b[39m); RAW.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m load_dotenv(); \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mALPHAVANTAGE_API_KEY loaded?\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mbool\u001b[39m(os.getenv(\u001b[33m'\u001b[39m\u001b[33mALPHAVANTAGE_API_KEY\u001b[39m\u001b[33m'\u001b[39m)))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import os, pathlib, datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "RAW = pathlib.Path('data/raw'); RAW.mkdir(parents=True, exist_ok=True)\n",
    "load_dotenv(); print('ALPHAVANTAGE_API_KEY loaded?', bool(os.getenv('ALPHAVANTAGE_API_KEY')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961f695",
   "metadata": {},
   "source": [
    "## Helpers (use or modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e262dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts():\n",
    "    return dt.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "def save_csv(df: pd.DataFrame, prefix: str, **meta):\n",
    "    mid = '_'.join([f\"{k}-{v}\" for k,v in meta.items()])\n",
    "    path = RAW / f\"{prefix}_{mid}_{ts()}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print('Saved', path)\n",
    "    return path\n",
    "\n",
    "def validate(df: pd.DataFrame, required):\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    return {'missing': missing, 'shape': df.shape, 'na_total': int(df.isna().sum().sum())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22fe6eb",
   "metadata": {},
   "source": [
    "## Part 1 — API Pull (Required)\n",
    "Choose an endpoint (e.g., Alpha Vantage or use `yfinance` fallback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f064f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m SYMBOL = \u001b[33m'\u001b[39m\u001b[33mAAPL\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m USE_ALPHA = \u001b[38;5;28mbool\u001b[39m(\u001b[43mos\u001b[49m.getenv(\u001b[33m'\u001b[39m\u001b[33mALPHAVANTAGE_API_KEY\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m USE_ALPHA:\n\u001b[32m      5\u001b[39m     url = \u001b[33m'\u001b[39m\u001b[33mhttps://www.alphavantage.co/query\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "SYMBOL = 'AAPL'\n",
    "USE_ALPHA = bool(os.getenv('ALPHAVANTAGE_API_KEY'))\n",
    "\n",
    "if USE_ALPHA:\n",
    "    url = 'https://www.alphavantage.co/query'\n",
    "    params = {\n",
    "        'function':'TIME_SERIES_DAILY_ADJUSTED',\n",
    "        'symbol': SYMBOL,\n",
    "        'outputsize':'compact',\n",
    "        'apikey': os.getenv('ALPHAVANTAGE_API_KEY')\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    key = [k for k in js if 'Time Series' in k][0]\n",
    "    df_api = pd.DataFrame(js[key]).T.reset_index().rename(\n",
    "        columns={'index':'date', '5. adjusted close':'adj_close'}\n",
    "    )[['date', 'adj_close']]\n",
    "    df_api['date'] = pd.to_datetime(df_api['date'])\n",
    "    df_api['adj_close'] = pd.to_numeric(df_api['adj_close'])\n",
    "else:\n",
    "    import yfinance as yf\n",
    "    df_api = yf.download(SYMBOL, period='3mo', interval='1d').reset_index()[['Date', 'Adj Close']]\n",
    "    df_api.columns = ['date', 'adj_close']\n",
    "\n",
    "v_api = validate(df_api, ['date', 'adj_close'])\n",
    "print(v_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = save_csv(df_api.sort_values('date'), prefix='api', source='alpha' if USE_ALPHA else 'yfinance', symbol=SYMBOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5078e753",
   "metadata": {},
   "source": [
    "## Part 2 — Scrape a Public Table (Required)\n",
    "Replace `SCRAPE_URL` with a permitted page containing a simple table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc7d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRAPE_URL = 'https://www.wsj.com/market-data'  # TODO: replace with permitted page\n",
    "headers = {'User-Agent':'AFE-Homework/1.0'}\n",
    "try:\n",
    "    resp = requests.get(SCRAPE_URL, headers=headers, timeout=30); resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    rows = [[c.get_text(strip=True) for c in tr.find_all(['th','td'])] for tr in soup.find_all('tr')]\n",
    "    header, *data = [r for r in rows if r]\n",
    "    df_scrape = pd.DataFrame(data, columns=header)\n",
    "except Exception as e:\n",
    "    print('Scrape failed, using inline demo table:', e)\n",
    "    html = '<table><tr><th>Ticker</th><th>Price</th></tr><tr><td>AAA</td><td>101.2</td></tr></table>'\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    rows = [[c.get_text(strip=True) for c in tr.find_all(['th','td'])] for tr in soup.find_all('tr')]\n",
    "    header, *data = [r for r in rows if r]\n",
    "    df_scrape = pd.DataFrame(data, columns=header)\n",
    "\n",
    "if 'Price' in df_scrape.columns:\n",
    "    df_scrape['Price'] = pd.to_numeric(df_scrape['Price'], errors='coerce')\n",
    "v_scrape = validate(df_scrape, list(df_scrape.columns)); v_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aba95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = save_csv(df_scrape, prefix='scrape', site='example', table='markets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282af07",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "- API Source: (URL/endpoint/params)\n",
    "- Scrape Source: (URL/table description)\n",
    "- Assumptions & risks: (rate limits, selector fragility, schema changes)\n",
    "- Confirm `.env` is not committed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b71f6-5015-45fb-a658-b9f8127c8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Sources & URLs\n",
    "Alpha Vantage API (if API key provided)\n",
    "URL: https://www.alphavantage.co/query\n",
    "Params: function=TIME_SERIES_DAILY_ADJUSTED, symbol=AAPL, outputsize=compact, apikey from .env\n",
    "\n",
    "Yahoo Finance (yfinance fallback)\n",
    "Uses yfinance Python package to download historical data for symbol AAPL for 3 months.\n",
    "\n",
    "Web Scraping Market Data Table\n",
    "Example URL: https://www.wsj.com/market-data\n",
    "HTML table parsed with BeautifulSoup (selectors for <tr>, <th>, <td>)\n",
    "\n",
    "Validation Logic\n",
    "Required columns for API & yfinance data: date, adj_close\n",
    "\n",
    "For scraped data: Check presence of columns (e.g., 'Ticker', 'Price'), convert 'Price' to numeric if applicable\n",
    "\n",
    "Validation output includes:\n",
    "\n",
    "Missing required columns list\n",
    "\n",
    "DataFrame shape (rows, columns)\n",
    "\n",
    "Total number of NA (missing) values\n",
    "\n",
    "Environment & Security\n",
    ".env file contains sensitive keys, e.g. ALPHAVANTAGE_API_KEY\n",
    "\n",
    "Ensure .env is listed in .gitignore to avoid committing secrets to version control\n",
    "\n",
    "Assumptions & Risks\n",
    "Assumptions:\n",
    "\n",
    "Alpha Vantage API key valid and rate limits not exceeded\n",
    "\n",
    "yfinance fallback availability and accuracy for historical prices\n",
    "\n",
    "Scraped websites’ structure remains stable and HTML format unchanged\n",
    "\n",
    "Risks:\n",
    "\n",
    "API limits or downtime could interrupt data retrieval\n",
    "\n",
    "Website layout changes can break scraping logic requiring updates\n",
    "\n",
    "Possible missing or corrupted data requiring manual review\n",
    "\n",
    "Sensitive API keys must be kept secure and not exposed publicly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
